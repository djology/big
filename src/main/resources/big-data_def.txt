大数据处理流程

        主要流程包括：数据收集、数据存储、数据处理、数据应用等环节

    数据收集
        数据的采集需要在多台服务器上进行，且不能影响正常业务的开展
        基于这种需求，衍生出多种日志收集工具，Flume、Logstash、Kibana等

    数据存储
        MySQL、Oracle等传统的关系型数据库，的优点是能够快速存储结构化的数据，支持随机访问
        但大数据的数据结构通常是半结构化（如日志数据），甚至是非结构化（如视频、音频数据）

        为了解决海量半结构化和非结构化数据的存储，衍生出Hadoop HDFS、KFS、GFS等分布式文件系统
        支持结构化、半结构化和非结构化数据的存储，并可以通过增加机器进行横向扩展

    数据分析
        大数据处理最重要的环节就是数据分析，数据分析通常是：批处理，流处理

        批处理，对一段时间内海量的离线数据进行统一的处理，对应的框架有Hadoop MapReduce、Spark、Flink等
        流处理，对运动中的数据进行处理，即在接收数据的同时就对其进行处理，对应的框架有Storm、Spark Streaming、Flink Streaming等

        为了让熟悉SQL的人员也能够进行数据的分析，查询分析框架应运而生
        常用的有Hive、Spark SQL、Flink SQL、Pig、Phoenix等

    数据应用
        可以将数据进行可视化展现，或者将数据用于优化推荐算法，又或是训练机器学习模型

    其他框架
        针对大数据处理中的各种复杂问题分别衍生出各类框架：
            单机的处理能力存在瓶颈，所以大数据框架都是采用集群模式进行部署，为了方便进行集群的部署、监控和管理
            衍生出Ambari、Cloudera Manager等集群管理工具

            ZooKeeper是最常用的分布式协调服务，能够解决大多数集群问题，包括首领选举、失败恢复、元数据存储及其一致性保证
            同时针对集群资源管理的需求，衍生出Hadoop YARN

            如何调度多个复杂且彼此存在依赖关系的作业，基于这种需求，产生了Azkaban和Oozie等工作流调度框架

            Kafka，可以用于消峰，避免在秒杀等场景下并发数据对流处理程序造成冲击

            Sqoop，主要解决的是数据迁移的问题，能够通过简单的命令将关系型数据库中的数据导入到HDFS、Hive，或反向行之